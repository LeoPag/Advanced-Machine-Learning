{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"task2_new.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"13-LI-IqYgdx1I_4QMQtP-7YSCBOxHuUi","authorship_tag":"ABX9TyMWBZtU2vvnUUOok/t8pr7F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Pdv1VEZ9qas-"},"source":["### **SETUP:**"]},{"cell_type":"code","metadata":{"id":"kHl5eTWWpywk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638779669307,"user_tz":-60,"elapsed":5634,"user":{"displayName":"Matteo Omenetti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmIulzltC6FDXap-CHSSAJBQ3ogsrWyIOAlg858w=s64","userId":"02799931472704688143"}},"outputId":"415dd244-ab9d-4f2f-f1c3-390c37ab72d2"},"source":["!pip install biosppy"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting biosppy\n","  Downloading biosppy-0.7.3.tar.gz (85 kB)\n","\u001b[?25l\r\u001b[K     |███▉                            | 10 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 20 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 30 kB 18.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 40 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 51 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 61 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 71 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 81 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 85 kB 2.5 MB/s \n","\u001b[?25hCollecting bidict\n","  Downloading bidict-0.21.4-py3-none-any.whl (36 kB)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from biosppy) (3.1.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from biosppy) (3.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from biosppy) (1.19.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from biosppy) (1.0.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from biosppy) (1.4.1)\n","Collecting shortuuid\n","  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from biosppy) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from biosppy) (1.1.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from biosppy) (4.1.2.30)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->biosppy) (1.5.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->biosppy) (3.0.6)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->biosppy) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->biosppy) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->biosppy) (2.8.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->biosppy) (3.0.0)\n","Building wheels for collected packages: biosppy\n","  Building wheel for biosppy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for biosppy: filename=biosppy-0.7.3-py2.py3-none-any.whl size=95430 sha256=f058237e4f0a37088d1df17049aa8464556b5f1b764ec302b7ba8d865b9fb86a\n","  Stored in directory: /root/.cache/pip/wheels/2f/4f/8f/28b2adc462d7e37245507324f4817ce1c64ef2464f099f4f0b\n","Successfully built biosppy\n","Installing collected packages: shortuuid, bidict, biosppy\n","Successfully installed bidict-0.21.4 biosppy-0.7.3 shortuuid-1.0.8\n"]}]},{"cell_type":"code","metadata":{"id":"ssA3MlGbp7Qe","executionInfo":{"status":"ok","timestamp":1638779671903,"user_tz":-60,"elapsed":2599,"user":{"displayName":"Matteo Omenetti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmIulzltC6FDXap-CHSSAJBQ3ogsrWyIOAlg858w=s64","userId":"02799931472704688143"}}},"source":["import matplotlib.pyplot as plt\n","import biosppy \n","from biosppy.signals import ecg\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import matplotlib\n","import tensorflow as tf\n","from tensorflow.keras import utils\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras import Input, Model, layers\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten\n","from tensorflow.keras import losses\n","from tensorflow import keras\n","from tensorflow.keras.callbacks import EarlyStopping\n","from sklearn.utils import class_weight\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import BaggingClassifier\n","import math\n","from scipy.stats import kurtosis\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import RepeatedKFold\n","from scipy.signal import butter, lfilter, welch\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from keras.callbacks import ModelCheckpoint\n","from sklearn.preprocessing import MinMaxScaler, RobustScaler\n","import pandas as pd\n","import scipy.io as sio\n","from os import listdir\n","from os.path import isfile, join\n","import numpy as np\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout, Conv1D, GlobalAveragePooling1D, MaxPooling1D\n","from keras import regularizers\n","from keras.utils import np_utils\n","\n"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xGQSmgZ5plHu"},"source":["### **PLOT ECG**"]},{"cell_type":"code","metadata":{"id":"d_dHej0D3-Aw","executionInfo":{"status":"ok","timestamp":1638779671904,"user_tz":-60,"elapsed":14,"user":{"displayName":"Matteo Omenetti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmIulzltC6FDXap-CHSSAJBQ3ogsrWyIOAlg858w=s64","userId":"02799931472704688143"}}},"source":["def plot_data(signal):\n","    # Some matplotlib setting \n","    plt.rcParams[\"figure.figsize\"] = (150, 15)\n","    plt.rcParams['lines.linewidth'] = 5\n","    plt.rcParams['xtick.labelsize'] = 24\n","    plt.rcParams['ytick.labelsize'] = 32\n","    plt.rcParams['axes.labelsize'] = 48\n","    plt.rcParams['axes.titlesize'] = 48\n","\n","    fig, axs = plt.subplots(1, 1)\n","    seconds = np.arange(0, 600) / 30 \n","\n","    # Get a subsequence of a signal and downsample it for visualization purposes\n","    measurements = signal[1000:7000:10] \n","    # convert volts to millivolts\n","    measurements /= 1000\n","    axs.plot(seconds, measurements, color='k', rasterized=True)\n","\n","    # hide tick and tick label of the big axes\n","    axs.spines['top'].set_visible(False)\n","    axs.spines['right'].set_visible(False)\n","    axs.spines['bottom'].set_visible(False)\n","    axs.spines['left'].set_visible(False)\n","    plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n","           \n","    plt.show()"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BJnu_DrmqezH"},"source":["### **UTILS ON FILES AND DATA:**"]},{"cell_type":"code","metadata":{"id":"CTVfdRPtqHc-","executionInfo":{"status":"ok","timestamp":1638779671904,"user_tz":-60,"elapsed":12,"user":{"displayName":"Matteo Omenetti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmIulzltC6FDXap-CHSSAJBQ3ogsrWyIOAlg858w=s64","userId":"02799931472704688143"}}},"source":["def write_X_to_file(file_name, X):\n","  f  = open(file_name, \"w\")\n","  s = \"id,\"\n","  for i in range(len(X[0])):\n","    s = s + \"x\" + str(i) + \",\"\n","  s = s[:-1] + \"\\n\"\n","  f.write(s)\n","  \n","  id = 0\n","  for x in X:\n","    list_string = ','.join(str(feature) for feature in x)\n","    f.write(str(id) + \",\" + list_string + \"\\n\")\n","    id = id + 1\n","  f.close()\n","\n","def write_y_to_file(file_name, Y):\n","  f  = open(file_name, \"w\")\n","  f.write(\"id,y\\n\")\n","  id = 0\n","  for y in Y:\n","    f.write(str(id) + \",\" + str(y) + \"\\n\")\n","    id = id + 1\n","  f.close()\n","\n","def write_output(file_name, y):\n","  write_y_to_file(file_name, y)\n","\n","def write_probs_to_file(file_name, y):\n","  f  = open(file_name, \"w\")\n","  f.write(\"id,y\\n\")\n","  id = 0\n","  for y in Y:\n","    f.write(str(id) + \",\" + str(y) + \"\\n\")\n","    id = id + 1\n","  f.close()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"h7esYHkKp_oo","executionInfo":{"status":"ok","timestamp":1638780144723,"user_tz":-60,"elapsed":361,"user":{"displayName":"Matteo Omenetti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmIulzltC6FDXap-CHSSAJBQ3ogsrWyIOAlg858w=s64","userId":"02799931472704688143"}}},"source":["def get_train_data():\n","   \n","    X = pd.read_csv('drive/MyDrive/AML_TASK_2/X_train.csv', float_precision='high').drop('id', axis=1)\n","    y = pd.read_csv('drive/MyDrive/AML_TASK_2/y_train.csv', float_precision='high').drop('id', axis=1)\n","\n","    return X, y\n","\n","def get_test_data():\n","   \n","    X = pd.read_csv('drive/MyDrive/AML_TASK_2/X_test.csv', float_precision='high').drop('id', axis=1)\n","\n","    return X\n","\n","def get_processed_train_data():\n","   \n","    X = pd.read_csv('drive/MyDrive/AML_TASK_2/X_features_train_puliti.csv', float_precision='high').drop('id', axis=1)\n","    y = pd.read_csv('drive/MyDrive/AML_TASK_2/y_train.csv', float_precision='high').drop('id', axis=1)\n","\n","    return X, y\n","\n","def get_processed_test_data():\n","   \n","    X = pd.read_csv('drive/MyDrive/AML_TASK_2/X_features_test_puliti.csv', float_precision='high').drop('id', axis=1)\n","  \n","    return X\n"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"sx8uXXRxqDAZ","executionInfo":{"status":"ok","timestamp":1638779671905,"user_tz":-60,"elapsed":11,"user":{"displayName":"Matteo Omenetti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmIulzltC6FDXap-CHSSAJBQ3ogsrWyIOAlg858w=s64","userId":"02799931472704688143"}}},"source":["def split_data_train_test(X, y, test_split=0.1):\n","    np.random.seed(54678)\n","\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_split, shuffle=True)\n","\n","    return X_train, X_test, y_train, y_test\n","\n","def split_data_train_test_validation(X, y, test_split=0.15, val_split=0.1):\n","    np.random.seed(90876)\n","    \n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_split, shuffle=True)\n","\n","    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_split, shuffle=True)\n","\n","    return np.array(X_train), np.array(X_test), np.array(X_val), np.array(y_train), np.array(y_test), np.array(y_val)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"4iV02FF8T1St","executionInfo":{"status":"ok","timestamp":1638779671905,"user_tz":-60,"elapsed":11,"user":{"displayName":"Matteo Omenetti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmIulzltC6FDXap-CHSSAJBQ3ogsrWyIOAlg858w=s64","userId":"02799931472704688143"}}},"source":["# Utils functions for cascade \n","def create_y(class1, class2, y_):\n","    y = np.copy(y_)\n","    for i, cl in enumerate(y):\n","        if cl[0] != class1:\n","            cl[0] = class2\n","    return y\n","\n","def remove_class(x_,y_, class1):\n","    x,y = np.copy(x_), np.copy(y_)\n","    mask = np.ones(len(y), dtype=bool)\n","    for i, cl in enumerate(y):\n","        if cl[0] == class1:\n","            mask[i] = False\n","    return x[mask], y[mask]\n","\n","def remove_class_test(x_,y_, class1):\n","    x,y = np.copy(x_), np.copy(y_)\n","    mask = np.ones(len(y), dtype=bool)\n","    for i, cl in enumerate(y):\n","        if cl[0] == class1:\n","            mask[i] = False\n","    return x[mask]\n","\n","def add_empty(y, empty):\n","    for i in empty:\n","        y = np.insert(y, i, 0)\n","    return y\n"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XDKm2Whz0ZDt"},"source":["### **UTILS:**"]},{"cell_type":"code","metadata":{"id":"KKTtNJzMqM7W","executionInfo":{"status":"ok","timestamp":1638779671906,"user_tz":-60,"elapsed":11,"user":{"displayName":"Matteo Omenetti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmIulzltC6FDXap-CHSSAJBQ3ogsrWyIOAlg858w=s64","userId":"02799931472704688143"}}},"source":["# remove NAN values from data\n","def no_nan(X):\n","    X_no_nan = []\n","    for i in range(len(X)):\n","        measurement = X.loc[i].dropna().to_numpy()\n","        X_no_nan.append(measurement)\n","    X_no_nan = np.array(X_no_nan)\n","\n","    return(X_no_nan)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"XEV1nqdj0gNj","executionInfo":{"status":"ok","timestamp":1638779671906,"user_tz":-60,"elapsed":10,"user":{"displayName":"Matteo Omenetti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmIulzltC6FDXap-CHSSAJBQ3ogsrWyIOAlg858w=s64","userId":"02799931472704688143"}}},"source":["# given an array return its min, max, mean, median and std\n","def properties(arr):\n","  a = np.amin(arr)\n","  b = np.amax(arr)\n","  c = np.mean(arr)\n","  d = np.median(arr)\n","  e = np.std(arr)\n","\n","  return a, b, c, d, e"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"B48WI88hqUPh","executionInfo":{"status":"ok","timestamp":1638779671907,"user_tz":-60,"elapsed":11,"user":{"displayName":"Matteo Omenetti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmIulzltC6FDXap-CHSSAJBQ3ogsrWyIOAlg858w=s64","userId":"02799931472704688143"}}},"source":["# process each ECG signal with biosspy\n","def process_biosppy(X):\n","    out_filtered = []\n","    out_templates = []\n","    r_peaks = []\n","    heart_rates_ts = []\n","    heart_rates_values = []\n","    for i in X:\n","      sig = ecg.ecg(signal=i, sampling_rate=300., show=False)\n","      out_filtered.append(sig['filtered'])\n","      out_templates.append(sig['templates'])\n","      r_peaks.append(sig['rpeaks'])\n","      heart_rates_ts.append(sig['heart_rate_ts'])\n","      heart_rates_values.append(sig['heart_rate'])\n","\n","    return np.array(out_filtered),  np.array(out_templates), np.array(r_peaks), np.array(heart_rates_ts), np.array(heart_rates_values)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qFc1MLdAy5Ei"},"source":["### **FEATURES EXTRACTION:**"]},{"cell_type":"code","metadata":{"id":"r5RKY85n8Dcy","executionInfo":{"status":"ok","timestamp":1638779671907,"user_tz":-60,"elapsed":10,"user":{"displayName":"Matteo Omenetti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmIulzltC6FDXap-CHSSAJBQ3ogsrWyIOAlg858w=s64","userId":"02799931472704688143"}}},"source":["def process_heart_rate(heart_rate_ts):\n","\n","  ts_distances = np.diff(heart_rate_ts)\n","  second_der = np.diff(ts_distances)\n","\n","  return ts_distances, second_der"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pun0Mda_8AqZ","executionInfo":{"status":"ok","timestamp":1638779671907,"user_tz":-60,"elapsed":10,"user":{"displayName":"Matteo Omenetti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmIulzltC6FDXap-CHSSAJBQ3ogsrWyIOAlg858w=s64","userId":"02799931472704688143"}}},"source":["def value_distance_ecg(ecg):\n","  diff = np.diff(ecg)\n","\n","  std = np.std(diff)\n","  kurt = kurtosis(diff)\n","  mean = np.mean(diff)\n","\n","  return std, kurt, mean"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"36Z4eR45JYc_","executionInfo":{"status":"ok","timestamp":1638779671908,"user_tz":-60,"elapsed":11,"user":{"displayName":"Matteo Omenetti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmIulzltC6FDXap-CHSSAJBQ3ogsrWyIOAlg858w=s64","userId":"02799931472704688143"}}},"source":["# detect p, q, r, s peaks for a given template of an ECG scan\n","def peaks_detection(template):\n","  P_peaks_values, P_peaks_indx, Q_peaks_values, Q_peaks_indx, R_peaks_values, S_peaks_values, S_peaks_indx, T_peaks_values, T_peaks_indx = [],[],[],[],[],[],[],[],[]\n","  for a in template:\n","      \n","      r_peak_indx = np.where(a==max(a[int(len(a)/4):int(len(a)/2)]))[0][0]\n","      r_peak_value = a[r_peak_indx]\n","      R_peaks_values.append(r_peak_value)\n","      first_half = a[:r_peak_indx]\n","\n","      q_indx = np.where(a==min(first_half[15:]))[0][0]\n","      Q_peaks_indx.append(q_indx)\n","      Q_peaks_values.append(a[q_indx])\n","\n","      first_half = first_half[:q_indx]\n","      p_indx = np.where(first_half==max(first_half))[0][0]\n","      P_peaks_indx.append(p_indx)\n","      P_peaks_values.append(a[p_indx])\n","\n","      second_half = a[r_peak_indx+1:] \n","      s_indx = np.where(a==min(second_half[0:20]))[0][0]\n","      S_peaks_indx.append(s_indx)\n","      S_peaks_values.append(a[s_indx])\n","\n","      end = a[s_indx+1:]\n","      t_indx = np.where(a==max(end))[0][0]\n","      T_peaks_indx.append(t_indx)\n","      T_peaks_values.append(a[t_indx])\n","\n","    \n","  return  np.array(P_peaks_values), np.array(P_peaks_indx), np.array(Q_peaks_values), np.array(Q_peaks_indx), np.array(R_peaks_values), np.array(S_peaks_values), np.array(S_peaks_indx), np.array(T_peaks_values), np.array(T_peaks_indx)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"rLWqqeS1XWCS","executionInfo":{"status":"ok","timestamp":1638779672262,"user_tz":-60,"elapsed":364,"user":{"displayName":"Matteo Omenetti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmIulzltC6FDXap-CHSSAJBQ3ogsrWyIOAlg858w=s64","userId":"02799931472704688143"}}},"source":["# given a list of templates (3d list), extarct a series of features for each ecg\n","def feature_extraction(templates):\n","  features = []\n","  counter = -1\n","  for template in templates:\n","    counter += 1\n","    feature = []\n","    \n","    P_peaks_values, P_peaks_indx, Q_peaks_values, Q_peaks_indx, R_peaks_values, S_peaks_values, S_peaks_indx, T_peaks_values, T_peaks_indx = peaks_detection(template)\n","\n","\n","    _, _, mean, _, std = properties(P_peaks_values)\n","    feature.append(std)\n","    feature.append(std/mean)\n","\n","    _, _, mean, _, std = properties(Q_peaks_values)\n","    feature.append(std)\n","    feature.append(std/mean)\n","\n","    _, _, mean, _, std = properties(R_peaks_values)\n","    feature.append(std)\n","    feature.append(std/mean)\n","\n","    _, _, mean, _, std = properties(S_peaks_values)\n","    feature.append(std)\n","    feature.append(std/mean)\n","\n","    _, _, mean, _, std = properties(T_peaks_values)\n","    feature.append(std)\n","    feature.append(std/mean)\n","\n","    qrs = S_peaks_indx - Q_peaks_indx\n","    feature.append(np.mean(qrs))\n","    feature.append(np.std(qrs))\n","\n","    pr = P_peaks_indx\n","    feature.append(np.mean(pr))\n","    feature.append(np.std(pr))\n","\n","    hear_rate_ts_distances, second_der = process_heart_rate(heart_rates_ts[counter])\n","    if (len(hear_rate_ts_distances) == 0):\n","      feature.append(0)\n","      feature.append(0)\n","      feature.append(0)\n","      feature.append(0)\n","    else:\n","      minimum, maximum, _, median, std = properties(hear_rate_ts_distances)\n","      feature.append(minimum)\n","      feature.append(maximum)\n","      feature.append(median)\n","      feature.append(std)\n","\n","    if (len(second_der) == 0):\n","      feature.append(0)\n","      feature.append(0)\n","      feature.append(0)\n","      feature.append(0)\n","    else:\n","      minimum, maximum, _, median, std = properties(second_der)\n","      feature.append(minimum)\n","      feature.append(maximum)\n","      feature.append(median)\n","      feature.append(std)\n","\n","\n","    if len(heart_rates_values[counter]) == 0:\n","      feature.append(0)\n","      feature.append(0)\n","      feature.append(0)\n","      feature.append(0)\n","    else:\n","      minimum, maximum, _, median, std = properties(heart_rates_values[counter])\n","      feature.append(minimum)\n","      feature.append(maximum)\n","      feature.append(median)\n","      feature.append(std)\n","\n","    std, kurt, mean = value_distance_ecg(X_processed[counter])\n","    feature.append(mean)\n","    feature.append(std)\n","    feature.append(kurt)\n","\n","    \n","    features.append(feature)\n","  return features"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"nld9lCAkUNx9","executionInfo":{"status":"ok","timestamp":1638779672263,"user_tz":-60,"elapsed":12,"user":{"displayName":"Matteo Omenetti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmIulzltC6FDXap-CHSSAJBQ3ogsrWyIOAlg858w=s64","userId":"02799931472704688143"}},"outputId":"008fdf5b-4885-4f7e-d528-42e42a7d5a54"},"source":["# Uncomment this if you want to load the train file, extrapolate the features\n","# and create the feature file\n","\n","\"\"\"\n","# get train data and create features file\n","X, y = get_train_data()\n","X = no_nan(X)\n","X_processed, templates, r_peaks,  heart_rates_ts, heart_rates_values = process_biosppy(X)\n","X_processed = np.array(X_processed)\n","templates = np.array(templates)\n","r_peaks = np.array(r_peaks)\n","features = feature_extraction(templates)\n","write_X_to_file(\"drive/MyDrive/AML_TASK_2/X_features_train.csv\", features)\n","\"\"\""],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n# get train data and create features file\\nX, y = get_train_data()\\nX = no_nan(X)\\nX_processed, templates, r_peaks,  heart_rates_ts, heart_rates_values = process_biosppy(X)\\nX_processed = np.array(X_processed)\\ntemplates = np.array(templates)\\nr_peaks = np.array(r_peaks)\\nfeatures = feature_extraction(templates)\\nwrite_X_to_file(\"drive/MyDrive/AML_TASK_2/X_features_train.csv\", features)\\n'"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"LeOer9jJPs8z","executionInfo":{"status":"ok","timestamp":1638779672264,"user_tz":-60,"elapsed":10,"user":{"displayName":"Matteo Omenetti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmIulzltC6FDXap-CHSSAJBQ3ogsrWyIOAlg858w=s64","userId":"02799931472704688143"}},"outputId":"0ff646e8-d9a6-431c-a282-2ce8e3f25628"},"source":["# Uncomment this if you want to load the test file, extrapolate the features\n","# and create the feature file\n","\"\"\"\n","# get test data and create features file \n","X_test = get_test_data()\n","X_test = no_nan(X_test)\n","X_processed, templates, r_peaks,  heart_rates_ts, heart_rates_values = process_biosppy(X_test)\n","X_processed = np.array(X_processed)\n","templates = np.array(templates)\n","r_peaks = np.array(r_peaks)\n","features = feature_extraction(templates)\n","write_X_to_file(\"drive/MyDrive/AML_TASK_2/X_features_test.csv\", features)\n","\"\"\""],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n# get test data and create features file \\nX_test = get_test_data()\\nX_test = no_nan(X_test)\\nX_processed, templates, r_peaks,  heart_rates_ts, heart_rates_values = process_biosppy(X_test)\\nX_processed = np.array(X_processed)\\ntemplates = np.array(templates)\\nr_peaks = np.array(r_peaks)\\nfeatures = feature_extraction(templates)\\nwrite_X_to_file(\"drive/MyDrive/AML_TASK_2/X_features_test.csv\", features)\\n'"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"u3KD_U1pqypf"},"source":["### **MODELS**"]},{"cell_type":"code","metadata":{"id":"2x1k9UBYrAnW","executionInfo":{"status":"ok","timestamp":1638780158967,"user_tz":-60,"elapsed":223,"user":{"displayName":"Matteo Omenetti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmIulzltC6FDXap-CHSSAJBQ3ogsrWyIOAlg858w=s64","userId":"02799931472704688143"}}},"source":["# get the processed train data\n","X, y = get_processed_train_data()\n","X = np.array(X)\n","y = np.array(y)\n","\n","# get the processed test data\n","X_TEST = get_processed_test_data()\n","X_TEST = np.array(X_TEST)"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"q9bI4tvBrBek","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638654669424,"user_tz":-60,"elapsed":4,"user":{"displayName":"Matteo Omenetti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmIulzltC6FDXap-CHSSAJBQ3ogsrWyIOAlg858w=s64","userId":"02799931472704688143"}},"outputId":"129f4811-fe42-4412-98dd-77b7767598bc"},"source":["# split the data into train test and validation\n","X_train, X_test, X_val, y_train, y_test, y_val = split_data_train_test_validation(X, y)\n","\n","# compute class weights to account for class imbalance\n","class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train.reshape(-1))\n","class_weights = {0: class_weights[0],\n","                 1: class_weights[1],\n","                 2: class_weights[2],\n","                 3: class_weights[3]\n","                 }\n","print('Training data: class weights {}'.format(class_weights))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training data: class weights {0: 0.4199570815450644, 1: 3.0673981191222572, 2: 0.8697777777777778, 3: 6.989285714285714}\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4beNcP1zao0D","executionInfo":{"status":"ok","timestamp":1638780214088,"user_tz":-60,"elapsed":52412,"user":{"displayName":"Matteo Omenetti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmIulzltC6FDXap-CHSSAJBQ3ogsrWyIOAlg858w=s64","userId":"02799931472704688143"}},"outputId":"e59d416d-6142-4172-980d-32d76a07c5cc"},"source":["\n","# perform one hot encoding\n","n_classes = 4\n","y = utils.to_categorical(y, n_classes)\n","\n","# perform standardization\n","# scaler = StandardScaler().fit(X_train)\n","# X_train = scaler.transform(X_train)\n","# X_test = scaler.transform(X_test) \n","\n","# create the neural network\n","model = Sequential()\n","model.add(Dense(200, activation='relu', input_dim=29))\n","model.add(Dense(200, activation='relu'))\n","model.add(Dense(200, activation='relu'))\n","model.add(Dense(4, activation='softmax'))\n","model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n","\n","model.summary()\n","# fit the model              \n","# model.fit(X_train, y_train, epochs=100, batch_size=15, verbose=0)\n","\n","# y_pred = model.predict(X_test)\n","# y_pred = np.argmax(y_pred, axis=1)\n","\n","# from sklearn.metrics import f1_score\n","# F1 = f1_score(y_test, y_pred, average='micro')\n","# print(F1)\n","\n","# # predict on the test data\n","# y_pred = model.predict(X_test)\n","# y_pred = np.argmax(y_pred, axis=1)\n","\n","scaler = StandardScaler().fit(X)\n","X = scaler.transform(X)\n","X_TEST = scaler.transform(X_TEST)  \n","\n","# fit the model              \n","model.fit(X, y, epochs=100, batch_size=15, verbose=0)\n","\n","y_pred = model.predict(X_TEST)\n","y_pred_no_prob = np.argmax(y_pred, axis=1)\n","\n","\n","write_output(\"drive/MyDrive/AML_TASK_2/nn.csv\", y_pred_no_prob)\n","write_output(\"drive/MyDrive/AML_TASK_2/nn_probs.csv\", y_pred)"],"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_16 (Dense)            (None, 200)               6000      \n","                                                                 \n"," dense_17 (Dense)            (None, 200)               40200     \n","                                                                 \n"," dense_18 (Dense)            (None, 200)               40200     \n","                                                                 \n"," dense_19 (Dense)            (None, 4)                 804       \n","                                                                 \n","=================================================================\n","Total params: 87,204\n","Trainable params: 87,204\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F5yXTXRvUCGQ","executionInfo":{"status":"ok","timestamp":1638654075042,"user_tz":-60,"elapsed":41524,"user":{"displayName":"Matteo Omenetti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmIulzltC6FDXap-CHSSAJBQ3ogsrWyIOAlg858w=s64","userId":"02799931472704688143"}},"outputId":"e4ee9c2d-1a14-4e0f-8157-6c8bb4f62d1b"},"source":["from xgboost import XGBClassifier\n","\n","xgb_model = XGBClassifier()\n","\n","cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=1)\n","n_scores = cross_val_score(xgb_model, X, y, scoring='f1_micro', cv=cv, n_jobs=-1, error_score='raise')\n","print('MAE: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n","\n","# from sklearn.metrics import f1_score\n","# F1 = f1_score(y_test, y_pred, average='micro')\n","# print(F1)\n","\n","xgb_model.fit(X, y)\n","y_pred = xgb_model.predict_proba(X_TEST)\n","write_output(\"drive/MyDrive/AML_TASK_2/xgb.csv\", y_pred)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MAE: 0.802 (0.013)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]}]},{"cell_type":"code","metadata":{"id":"15CrCQt5OSOl"},"source":["# -------------------------------------------------\n","# -------------------- CASCADE --------------------\n","# -------------------------------------------------\n","\n","## Train the models:\n","# First: 3 vs rest\n","y_3_rest = create_y(3, 4, y)\n","gbc_3_rest = GradientBoostingClassifier(n_estimators=250, learning_rate=0.2, random_state=0)\n","gbc_3_rest.fit(X, y_3_rest)\n","\n","# Second: 0 vs 1,2\n","x_3, y_3 = remove_class(X, y, 3)\n","y_0_rest = create_y(0, 4, y_3)\n","gbc_0_rest = GradientBoostingClassifier(n_estimators=250, learning_rate=0.2, random_state=0)\n","gbc_0_rest.fit(x_3, y_0_rest)\n","\n","# Third : 1 vs 2\n","x_03, y_03 = remove_class(x_3, y_3, 0)\n","y_1_2 = create_y(1, 4, y_03)\n","gbc_1_2 = GradientBoostingClassifier(n_estimators=250, learning_rate=0.2, random_state=0)\n","gbc_1_2.fit(x_03, y_1_2)\n","\n","\n","# Predict TEST using cascade \n","y_pred = np.ones(len(X_TEST), dtype=int)\n","for i, x in enumerate(X_TEST):\n","    prediction = gbc_3_rest.predict([x])\n","    \n","    if prediction == 3:\n","        y_pred[i] = 3\n","        continue\n","    \n","    prediction = gbc_0_rest.predict([x])\n","    if prediction == 0:\n","        y_pred[i] = 0\n","        continue\n","    \n","    prediction = gbc_1_2.predict([x])\n","    if prediction == 1: \n","      y_pred[i] = 1\n","    else:\n","      y_pred[i] = 2\n","\n","write_output(\"drive/MyDrive/AML_TASK_2/cascade.csv\", y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vkJJRGUXrJBx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638700972295,"user_tz":-60,"elapsed":4659,"user":{"displayName":"Matteo Omenetti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmIulzltC6FDXap-CHSSAJBQ3ogsrWyIOAlg858w=s64","userId":"02799931472704688143"}},"outputId":"527532e3-3814-4608-f3b9-98858ed1f3d7"},"source":["# ------------------------------------------------\n","# -------------------- FOREST --------------------\n","# ------------------------------------------------\n","\n","# Create the model with 100 trees\n","model = RandomForestClassifier(n_estimators=150)\n","\"\"\"\n","cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n","n_scores = cross_val_score(model, X, y, scoring='f1_micro', cv=cv, n_jobs=-1, error_score='raise')\n","print('MAE: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n","\"\"\"\n","\n","# Uncomment this if you want to train the forest on the entire train data set\n","# and create the output file for the REAL test data\n","# perform one hot encoding\n","# n_classes = 4\n","# y_train = utils.to_categorical(y, n_classes)\n","model.fit(X, y)\n","# predict on the test data\n","y_pred = model.predict_proba(X_TEST)\n","# print(y_pred)\n","\n","y_pred_new = []\n","for i in y_pred:\n","  inner = []\n","  for j in i:\n","    j = float(j)\n","    inner.append(j)\n","  y_pred_new.append(inner)\n","\n","write_output(\"drive/MyDrive/AML_TASK_2/forest_probs.csv\", y_pred_new)\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"]}]},{"cell_type":"markdown","metadata":{"id":"tnIVIrqjzTpu"},"source":["### **CONVOLUTIONAL NEURAL NETWORK**"]},{"cell_type":"code","metadata":{"id":"uqqsS-k83HUF"},"source":["# get train data\n","X, y = get_train_data()\n","X = no_nan(X)\n","X, templates, r_peaks,  heart_rates_ts, heart_rates_values = process_biosppy(X)\n","X = np.array(X)\n","y = np.array(y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G98X5f2t1GRU"},"source":["# standardize it\n","X_new = []\n","for  x in X:\n","  mean_len += len(x)\n","  x = (x - np.mean(x)) / np.std(x)\n","  X_new.append(x)\n","X_new = np.array(X_new)\n","\n","# pad with 0s\n","signal_lengths = [len(signal) for signal in X_new]\n","max_length = np.amax(signal_lengths)\n","X_new_new = []\n","for x in X_new:\n","  padding = max_length - len(x)\n","  x = np.hstack((x, [0] * padding))\n","  X_new_new.append(x)\n","\n","X_new_new = np.array(X_new_new)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k3e7Mi_p3RIV"},"source":["# get test data\n","X_test = get_test_data()\n","X_test = no_nan(X_test)\n","X_test, templates, r_peaks,  heart_rates_ts, heart_rates_values = process_biosppy(X_test)\n","X_test = np.array(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uY2gic_z3TMH"},"source":["# standardize it\n","X_new_test = []\n","for x in X_test:\n","  x = (x - np.mean(x)) / np.std(x)\n","  X_new_test.append(x)\n","X_new_test = np.array(X_new_test)\n","\n","# pad with 0s\n","X_new_new_test = []\n","for x in X_new_test:\n","  padding = 17841 - len(x)\n","  x = np.hstack((x, [0] * padding))\n","  X_new_new_test.append(x)\n","\n","X_new_new_test = np.array(X_new_new_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ttx5D5vxIJac"},"source":["# --------------------------------------------------------\n","# ------------------------- CNN -------------------------\n","# --------------------------------------------------------\n","\n","# perform one hot encoding\n","n_classes = 4\n","y_train = utils.to_categorical(y, n_classes)\n","\n","# create the model\n","model = Sequential()\n","model.add(Conv1D(128, 55, activation='relu', input_shape=(17841, 1)))\n","model.add(MaxPooling1D(10))\n","model.add(Dropout(0.5))\n","model.add(Conv1D(128, 25, activation='relu'))\n","model.add(MaxPooling1D(5))\n","model.add(Dropout(0.5))\n","model.add(Conv1D(128, 10, activation='relu'))\n","model.add(MaxPooling1D(5))\n","model.add(Dropout(0.5))\n","model.add(Conv1D(128, 5, activation='relu'))\n","model.add(GlobalAveragePooling1D())\n","# model.add(Flatten())\n","model.add(Dense(256, kernel_initializer='normal', activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(128, kernel_initializer='normal', activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(64, kernel_initializer='normal', activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(4, kernel_initializer='normal', activation='softmax'))\n","\n","model.summary()\n","\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","model.fit(X_new_new, y_train, batch_size=235, epochs=50, verbose=2, shuffle=True)\n","\n","y_pred = model.predict(X_new_new_test)\n","y_pred_no_prob = np.argmax(y_pred, axis=1)\n","\n","write_output(\"drive/MyDrive/AML_TASK_2/cnn.csv\", y_pred_no_prob)\n","write_output(\"drive/MyDrive/AML_TASK_2/cnn_probs.csv\", y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pkfplxS8oNdw"},"source":["# Perform a voting --> use the predictions from different models to create a new\n","# set of predictions\n","def democracy(file_names):\n","\n","  f_output = open(\"drive/MyDrive/AML_TASK_2/democracy.csv\", \"w\")\n","  files = []\n","  # opene all the files passed as inputs\n","  for file_name in file_names:\n","      f = open(file_name, \"r\")\n","      files.append(f)\n","  \n","  # discard all headers\n","  for f in files:\n","      header = f.readline()\n","  f_output.write(header)\n","\n","  for _ in range(3411):\n","      readings = []\n","      id = 0\n","      class_counter = [0] * 4\n","      for f in files:\n","          line = f.readline()\n","\n","          id = line.split(',')[0]\n","          reading = int(line.split(',')[1])\n","          class_counter[reading] = class_counter[reading] + 1\n","\n","      dem = class_counter.index(max(class_counter))\n","\n","      f_output.write(id + \",\" + str(dem) + \"\\n\")\n","\n","  for f in files:\n","      f.close()\n","  \n","  f_output.close()\n","\n","# Perform a voting --> use the probabilities of the predictions from different\n","# models to create a new set of predictions\n","def democracy_probs(file_names):\n","\n","  f_output = open(\"drive/MyDrive/AML_TASK_2/democracy_probs.csv\", \"w\")\n","  files = []\n","  # opene all the files passed as inputs\n","  for file_name in file_names:\n","      f = open(file_name, \"r\")\n","      files.append(f)\n","  \n","  # discard all headers\n","  for f in files:\n","      header = f.readline()\n","  f_output.write(header)\n","\n","  for _ in range(3411):\n","      readings = []\n","      id = 0\n","\n","      total_probs = [0] * 4\n","      for i, f in enumerate(files):\n","          line = f.readline()\n","\n","          id = line.split(',')[0]\n","          if i == 1:\n","            reading = line.split(',')[1:]\n","          else:\n","            reading = line.split(',')[1]\n","            reading = reading.split(' ')\n","          reading =  [x for x in reading if x != '']\n","          reading[0] = (reading[0])[1:]\n","          reading[3] = (reading[3].strip())[:-1]\n","          \n","          if (len(reading) > 4):\n","            reading = reading[:4]\n","\n","\n","          # reading = list(filter('', reading))\n","          # print(reading)\n","          \n","          \n","          reading = np.array(reading, dtype=np.float32)\n","          \n","          \n","          if i == 3:\n","            reading = 1/2 * reading\n","          else:\n","            reading = 1/6 * reading\n","          total_probs += reading\n","          # print(total_probs)\n","\n","      dem = np.argmax(total_probs)\n","      # print(dem)\n","      f_output.write(id + \",\" + str(dem) + \"\\n\")\n","\n","  for f in files:\n","      f.close()\n","  \n","  f_output.close()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9zP-_keRoRyc"},"source":["# This is vai democracy works\n","# file_names = ['drive/MyDrive/AML_TASK_2/cascade.csv', 'drive/MyDrive/AML_TASK_2/forest.csv', 'drive/MyDrive/AML_TASK_2/mlp.csv', 'drive/MyDrive/AML_TASK_2/cnn.csv', 'drive/MyDrive/AML_TASK_2/svc.csv']\n","# democracy(file_names)\n","\n","file_names = ['drive/MyDrive/AML_TASK_2/nn_probs.csv', 'drive/MyDrive/AML_TASK_2/forest_probs.csv', 'drive/MyDrive/AML_TASK_2/xgb_probs.csv', 'drive/MyDrive/AML_TASK_2/cnn_probs_padding.csv']\n","democracy_probs(file_names)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F44s_l7pIyZI"},"source":[""]}]}